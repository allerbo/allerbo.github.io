<style>
.container {
  display: flex;
  align-items: center;
  justify-content: left
}

img {
  max-width: 100%;
  max-height:100%;
}

.text {
  font-size: 16px;
  padding-left: 0px;
  padding-right: 30px;
}
</style>

  <body>
    <div class="container">
      <div class="text">
        <head>
          <title>Oskar Allerbo</title>
        </head>
        <body>
        <h2>Oskar Allerbo</h2>
        <p>Post Doctoral Researcher, Mathematical Statistics</p>
        <p><em>Department of Mathematicals<br>
        KTH Royal Institute of Technology<br>
        100 44 STOCKHOLM<br>
        SWEDEN<br>
        <br>
        email: oallerbo(at)kth.se</em></p>
      </div>
      <div class="image">
        <img src="oskar1.jpg">
      </div>
    </div>
  </body>


<p>I am interested in many different aspects of statistics/ML, including generalization, implicit and explicit reguralization, artificial neural networks, kernel methods, and sparsity.</p>
<!-- <a href="CV_Oskar_Allerbo.pdf">CV</a><br> -->
<!-- Doctoral Thesis: <a href="https://gupea.ub.gu.se/handle/2077/77367">Efficient training of interpretable, non-linear regression models</a>.<br> -->
<h3>Publications and Preprints</h3>
Allerbo, O., and Schön, T. B. (2025). 
<a href="https://arxiv.org/abs/2505.11006">Supervised Models Can Generalize Also When Trained on Random Labels.</a> 
<i>arXiv:2505.11006.</i>
<a href="https://github.com/allerbo/training_without_y">Code.</a><br>
Allerbo, O. (2025). 
<a href="https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-19/issue-1/Fast-robust-kernel-regression-through-sign-gradient-descent-with-early/10.1214/25-EJS2361.full">Fast Robust Kernel Regression through Sign Gradient Descent with Early Stopping.</a> 
<i>Electronic Journal of Statistics</i>, 19(1), 1231-1285.
<a href="https://github.com/allerbo/fast_robust_kernel_regression">Code.</a><br>
Allerbo, O. (2023). 
<a href="https://arxiv.org/abs/2311.01762">Changing the Kernel During Training Leads to Double Descent in Kernel Regression.</a> 
<i>arXiv:2311.01762.</i>
<a href="https://github.com/allerbo/non_constant_kgd">Code.</a><br>
Allerbo, O., Jonasson, J., and Jörnsten, R. (2023). 
<a href="https://jmlr.org/papers/v24/22-0119.html">Elastic Gradient Descent, an Iterative Optimization Method Approximating the Solution Paths of the Elastic Net.</a> 
<i>Journal of Machine Learning Research</i>, 24(277), 1-53.
<a href="https://github.com/allerbo/elastic_gradient_descent">Code.</a><br>
Allerbo, O., and Jörnsten, R. (2022). 
<a href="https://arxiv.org/abs/2205.11956">Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control.</a> 
<i>arXiv:2205.11956.</i>
<a href="https://github.com/allerbo/jacobian_bandwidth_selection">Code.</a><br>
Allerbo, O., and Jörnsten, R. (2021). 
<a href="https://jmlr.org/papers/v22/21-0203.html">Non-linear, Sparse Dimensionality Reduction via Path Lasso Penalized Autoencoders.</a> 
<i>Journal of Machine Learning Research</i>, 22(283), 1-28.
<a href="https://github.com/allerbo/path_lasso">Code.</a><br>
Allerbo, O., and Jörnsten, R. (2022). 
<a href="https://link.springer.com/article/10.1007/s00180-021-01190-4">Flexible, Non-parametric Modeling Using Regularized Neural Networks.</a> 
<i>Computational Statistics</i>, 37(4), 2029-2047.</a>
<a href="https://github.com/allerbo/prada_net">Code.</a><br>
<br>
</body>
</html>

